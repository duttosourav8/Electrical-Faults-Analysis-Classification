# -*- coding: utf-8 -*-
"""Electrical Faults Analysis & Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s8eqhyYjlLcY7dFnbVD6_p97S2gLK7Kx
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import warnings

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

sns.set_style('darkgrid')
plt.rcParams['figure.figsize'] = (13,9)
plt.rcParams['font.size'] = 20
warnings.filterwarnings('ignore')

df1 = pd.read_csv('/content/detect_dataset.csv')
df2 = pd.read_csv('/content/classData.csv')

df1.info()

df1.head()

df1.isnull().any()



df1.shape

df1['Output (S)'].value_counts()

import matplotlib.pyplot as plt

# Data for the bar chart
categories = ['Output (S) = 0', 'Output (S) = 1']
counts = [6505, 5496]

# Create a bar chart
plt.figure(figsize=(8, 6))
plt.bar(categories, counts, color=['skyblue', 'salmon'])

# Add labels and title

plt.xlabel('Output (S)', fontsize=14, fontweight='bold')
plt.ylabel('Count', fontsize=14, fontweight='bold')

# Add the count labels above the bars
for i, count in enumerate(counts):
    plt.text(i, count + 50, str(count), ha='center', fontsize=12, fontweight='bold')

# Show the plot
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

models = ["RandomForest", "LogisticRegression", "SVM",
          "GradientBoosting", "KNN", "DecisionTree"]
accuracies = [0.9967, 0.6064, 0.9808, 0.9946, 0.9965, 0.9962]

plt.figure(figsize=(8, 6))
bars = plt.bar(models, accuracies, color='skyblue')

# Add accuracy values
for bar in bars:
    h = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, h + 0.01, f"{h:.4f}",
             ha='center', va='bottom', fontsize=10, fontweight='bold')

plt.ylabel("Accuracy", fontsize=12, fontweight='bold')
plt.ylim(0, 1.05)
plt.xticks(rotation=30, ha='right')   # <-- Rotation fix
plt.grid(axis='y', linestyle='--', alpha=0.6)

plt.tight_layout()
plt.show()

df2.info()

df2.head()

any(df2.isna().sum()>0)

df1.drop(df1.iloc[:,[7,8]], axis=1, inplace=True)

df1.shape

df2.shape

sns.set_theme(context='notebook',
              style='white',
              palette='deep',
              font='Lucida Calligraphy',
              font_scale=1.5,
              color_codes=True,
              rc=None)

import matplotlib

plt.rcParams['figure.figsize'] = (14,8)
plt.rcParams['figure.facecolor'] = '#F0F8FF'
plt.rcParams['figure.titlesize'] = 'medium'
plt.rcParams['figure.dpi'] = 150
plt.rcParams['figure.edgecolor'] = 'green'
plt.rcParams['figure.frameon'] = True

plt.rcParams["figure.autolayout"] = True

plt.rcParams['axes.facecolor'] = '#F5F5DC'
plt.rcParams['axes.titlesize'] = 25
plt.rcParams["axes.titleweight"] = 'normal'
plt.rcParams["axes.titlecolor"] = 'Olive'
plt.rcParams['axes.edgecolor'] = 'pink'

plt.rcParams["axes.linewidth"] = 2
plt.rcParams["axes.grid"] = True
plt.rcParams['axes.titlelocation'] = 'center'
plt.rcParams["axes.labelsize"] = 20
plt.rcParams["axes.labelpad"] = 2
plt.rcParams['axes.labelweight'] = 1
plt.rcParams["axes.labelcolor"] = 'Olive'
plt.rcParams["axes.axisbelow"] = False
plt.rcParams['axes.xmargin'] = .2
plt.rcParams["axes.ymargin"] = .2

plt.rcParams["xtick.bottom"] = True
plt.rcParams['xtick.color'] = '#A52A2A'
plt.rcParams["ytick.left"] = True
plt.rcParams['ytick.color'] = '#A52A2A'

plt.rcParams['axes.grid'] = True
plt.rcParams['grid.color'] = 'green'
plt.rcParams['grid.linestyle'] = '--'
plt.rcParams['grid.linewidth'] = .5
plt.rcParams['grid.alpha'] = .3

plt.rcParams['legend.loc'] = 'best'
plt.rcParams['legend.facecolor'] =  'NavajoWhite'
plt.rcParams['legend.edgecolor'] = 'pink'
plt.rcParams['legend.shadow'] = True
plt.rcParams['legend.fontsize'] = 20


plt.rcParams['font.family'] = 'Lucida Calligraphy'
plt.rcParams['font.size'] = 14

plt.rcParams['figure.dpi'] = 150
plt.rcParams['figure.edgecolor'] = 'Blue'

ax = plt.subplot(1,2,1)
ax = sns.countplot(x='G', data=df2)
ax.bar_label(ax.containers[0])
plt.title("Ground Fault", fontsize=20,color = 'Brown',font='Lucida Calligraphy',pad=20)

ax =plt.subplot(1,2,2)
ax=df2['G'].value_counts().plot.pie(explode=[0.1, 0.1],autopct='%1.2f%%',shadow=True);
ax.set_title(label = "Ground Fault", fontsize = 20,color='Brown',font='Lucida Calligraphy',pad=20);

map16 = {
    "0000":"No Fault",

    "1000":"Ground Fault",
    "0001":"Fault in Line A",
    "0010":"Fault in Line B",
    "0100":"Fault in Line C",

    "1001":"LG fault (A-G)",
    "1010":"LG fault (B-G)",
    "1100":"LG fault (C-G)",

    "0011":"LL fault (A-B)",
    "0101":"LL fault (A-C)",
    "0110":"LL fault (B-C)",

    "1011":"LLG Fault (A-B-G)",
    "1101":"LLG Fault (A-C-G)",
    "1110":"LLG Fault (B-C-G)",

    "0111":"LLL Fault",
    "1111":"LLLG fault"
}
df2["faultType"] = df2[["G","C","B","A"]].astype(int).astype(str).agg("".join, axis=1).map(map16)



df2['faultType'].value_counts()

import matplotlib.pyplot as plt

# Fault Types and their corresponding counts
fault_types = ['No Fault', 'LLG Fault (A-B-G)', 'LLLG fault', 'LG fault (A-G)', 'LLL Fault', 'LL fault (B-C)']
counts = [2365, 1134, 1133, 1129, 1096, 1004]

# Set up the plot
plt.figure(figsize=(10, 6))

# Create the bar chart
bars = plt.bar(fault_types, counts, color='skyblue')

# Add labels and title
plt.xlabel('Fault Types', fontsize=14, fontweight='bold')
plt.ylabel('Count', fontsize=14, fontweight='bold')

# Rotate x-axis labels for better readability
plt.xticks(rotation=45, ha='right')

# Add the count value on top of each bar
for bar in bars:
    yval = bar.get_height()  # Get the height of the bar (i.e., the count)
    plt.text(bar.get_x() + bar.get_width()/2, yval + 20,  # Position the text slightly above the bar
             int(yval), ha='center', fontsize=12, fontweight='bold')

# Show gridlines
plt.grid(True, linestyle='--', linewidth=0.5)

# Save the figure as an image
plt.tight_layout()


# Show the plot
plt.show()

plt.figure(figsize = (10,4))
plt.plot(df2["Ia"])
plt.plot(df2["Ib"])
plt.plot(df2["Ic"]);

plt.figure(figsize = (10,4))
plt.plot(df2["Va"])
plt.plot(df2["Vb"])
plt.plot(df2["Vc"]);

print(df2['faultType'].unique())

No_Fault = df2[df2['faultType'] == 'No Fault']
No_Fault.sample(5).style.set_properties(**{'background-color': 'blue',
                                           'color': 'white',
                                           'border-color': 'darkblack'})

ax = plt.figure(figsize = (18,3))
ax = plt.plot(No_Fault["Ia"],'r')
ax = plt.plot(No_Fault["Ib"],'b')
ax = plt.plot(No_Fault["Ic"],'g');

ax = plt.figure(figsize = (18,3))
ax = plt.plot(No_Fault["Va"],'r')
ax = plt.plot(No_Fault["Vb"],'b')
ax = plt.plot(No_Fault["Vc"],'g');

# List of fault types to filter on
fault_types = ['LG fault (A-G)', 'LLG Fault (A-B-G)', 'LL fault (B-C)', 'LLL Fault', 'LLLG fault', 'No Fault']

# Loop over each fault type to filter and style the sample
for fault in fault_types:
    fault_df = df2[df2['faultType'] == fault]  # Filter for each fault type
    if not fault_df.empty:  # Check if the filtered DataFrame is not empty
        styled_sample = fault_df.sample(5).style.set_properties(**{
            'background-color': 'blue',
            'color': 'white',
            'border-color': 'darkblack'
        })
        # Display the styled sample for each fault type (optional)
        display(styled_sample)
    else:
        print(f"No records found for '{fault}'.")

import matplotlib.pyplot as plt

# List of fault types to filter on
fault_types = ['LG fault (A-G)', 'LLG Fault (A-B-G)', 'LL fault (B-C)', 'LLL Fault', 'LLLG fault', 'No Fault']

# Loop over each fault type to filter, style the sample, and plot
for fault in fault_types:
    fault_df = df2[df2['faultType'] == fault]  # Filter for each fault type
    if not fault_df.empty:  # Check if the filtered DataFrame is not empty
        # Display the sample with styling (LIME-style)
        styled_sample = fault_df.sample(5).style.set_properties(**{
            'background-color': 'blue',
            'color': 'white',
            'border-color': 'darkblack'
        })
        # Display the styled sample
        display(styled_sample)

        # Create the line plot for each fault type (different color for each component)
        plt.figure(figsize=(18, 3))

        # Plot each component with different colors (Red for Ia, Blue for Ib, Green for Ic)
        plt.plot(fault_df["Ia"], 'r', label='Ia')
        plt.plot(fault_df["Ib"], 'b', label='Ib')
        plt.plot(fault_df["Ic"], 'g', label='Ic')

        # Add title and labels
        plt.title(f'Fault Type: {fault} - Current Signals (Ia, Ib, Ic)', fontsize=16, fontweight='bold')
        plt.xlabel('Index', fontsize=14, fontweight='bold')
        plt.ylabel('Current (A)', fontsize=14, fontweight='bold')

        # Show legend
        plt.legend()

        # Display the plot
        plt.grid(True)
        plt.tight_layout()
        plt.show()

        # Plot for Voltage signals (Red for Va, Blue for Vb, Green for Vc)
        plt.figure(figsize=(18, 3))

        # Plot voltage components
        plt.plot(fault_df["Va"], 'r', label='Va')
        plt.plot(fault_df["Vb"], 'b', label='Vb')
        plt.plot(fault_df["Vc"], 'g', label='Vc')

        # Add title and labels
        plt.title(f'Fault Type: {fault} - Voltage Signals (Va, Vb, Vc)', fontsize=16, fontweight='bold')
        plt.xlabel('Index', fontsize=14, fontweight='bold')
        plt.ylabel('Voltage (V)', fontsize=14, fontweight='bold')

        # Show legend
        plt.legend()

        # Display the plot
        plt.grid(True)
        plt.tight_layout()
        plt.show()

    else:
        print(f"No records found for '{fault}'.")



df2['faultType'].shape

print(df2["faultType"].value_counts(dropna=False))
print("\nPercentages:\n", df2["faultType"].value_counts(normalize=True, dropna=False)*100)

feat = ["Ia","Ib","Ic","Va","Vb","Vc"]

X1, y1 = df1[feat], df1["Output (S)"].astype(int)
X1_tr, X1_te, y1_tr, y1_te = train_test_split(X1, y1, test_size=0.2, random_state=1, stratify=y1)

from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
detector = Pipeline([
    ("scaler", StandardScaler()),
    ("clf", RandomForestClassifier(n_estimators=300, random_state=1, n_jobs=-1))
])
detector.fit(X1_tr, y1_tr)

y1_pred = detector.predict(X1_te)
print("DETECTION REPORT")
print(classification_report(y1_te, y1_pred, digits=4))

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier

# Create a dictionary of classifiers to use for detection
models = {
    "RandomForest": Pipeline([
        ("scaler", StandardScaler()),
        ("clf", RandomForestClassifier(n_estimators=300, random_state=1, n_jobs=-1))
    ]),

    "LogisticRegression": Pipeline([
        ("scaler", StandardScaler()),
        ("clf", LogisticRegression(max_iter=2000, class_weight="balanced"))
    ]),

    "SVM": Pipeline([
        ("scaler", StandardScaler()),
        ("clf", SVC(kernel='rbf', probability=True, class_weight="balanced"))
    ]),

    "GradientBoosting": Pipeline([
        ("scaler", StandardScaler()),
        ("clf", GradientBoostingClassifier(random_state=1))
    ]),

    "KNN": Pipeline([
        ("scaler", StandardScaler()),
        ("clf", KNeighborsClassifier(n_neighbors=5))
    ]),

    "DecisionTree": Pipeline([
        ("scaler", StandardScaler()),
        ("clf", DecisionTreeClassifier(random_state=1, class_weight="balanced"))
    ]),
}

# Loop over each model, fit it, and print the classification report
for model_name, model_pipeline in models.items():
    model_pipeline.fit(X1_tr, y1_tr)  # Fit the model on training data
    y1_pred = model_pipeline.predict(X1_te)  # Predict on test data

    print(f"\n{model_name} — TEST REPORT")
    print(classification_report(y1_te, y1_pred, digits=4))

import pandas as pd
import matplotlib.pyplot as plt

# Data: Classification results for each model
data = {
    "Model": [
        "RandomForest", "LogisticRegression", "SVM",
        "GradientBoosting", "KNN", "DecisionTree"
    ],
    "Accuracy": [0.9967, 0.6064, 0.9808, 0.9946, 0.9967, 0.9967],
    "Macro Avg Precision": [0.9965, 0.6039, 0.9826, 0.9945, 0.9964, 0.9966],
    "Macro Avg Recall": [0.9968, 0.6041, 0.9793, 0.9946, 0.9969, 0.9967],
    "Macro Avg F1-Score": [0.9966, 0.6040, 0.9806, 0.9945, 0.9966, 0.9966],
    "Weighted Avg Precision": [0.9967, 0.6068, 0.9813, 0.9946, 0.9967, 0.9967],
    "Weighted Avg Recall": [0.9967, 0.6064, 0.9808, 0.9946, 0.9967, 0.9967],
    "Weighted Avg F1-Score": [0.9967, 0.6066, 0.9808, 0.9946, 0.9967, 0.9967]
}

# Create the dataframe
df_results = pd.DataFrame(data)

# Select only the columns from 'Accuracy' to 'Weighted Avg F1-Score'
df_results = df_results[['Model', 'Accuracy', 'Macro Avg Precision', 'Macro Avg Recall',
                         'Macro Avg F1-Score', 'Weighted Avg Precision', 'Weighted Avg Recall', 'Weighted Avg F1-Score']]

# Convert numeric columns to float, ensuring the comparison works correctly
numeric_cols = df_results.columns[1:]  # all columns except 'Model'
df_results[numeric_cols] = df_results[numeric_cols].apply(pd.to_numeric, errors='coerce')

# Plotting the table as an image
fig, ax = plt.subplots(figsize=(12, 6))  # Set the size of the figure

# Hide axes
ax.xaxis.set_visible(False)
ax.yaxis.set_visible(False)

# Create the table and add it to the axes
ax.table(cellText=df_results.values, colLabels=df_results.columns, loc='center', cellLoc='center', colColours=["#f2f2f2"]*len(df_results.columns))

# Save the table as an image
plt.savefig('/content/classification_report_table.png', bbox_inches='tight', dpi=300)

# Show the plot (Optional)
plt.show()

print("Table saved as 'classification_report_table.png'")

import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_curve, auc
from sklearn import metrics

# Assuming X1_tr, X1_te, y1_tr, and y1_te are already defined

# Create the RandomForest model pipeline
random_forest_pipeline = Pipeline([
    ("scaler", StandardScaler()),
    ("clf", RandomForestClassifier(n_estimators=300, random_state=1, n_jobs=-1))
])

# 1. **Train the model and print the classification report**
random_forest_pipeline.fit(X1_tr, y1_tr)  # Fit the model on training data
y1_pred = random_forest_pipeline.predict(X1_te)  # Predict on test data

# Print Classification Report
print(f"RandomForest — TEST REPORT")
print(classification_report(y1_te, y1_pred, digits=4))

# 2. **Confusion Matrix**
cm = confusion_matrix(y1_te, y1_pred)
print(f"Confusion Matrix:\n{cm}")

# Plot confusion matrix as a heatmap
plt.figure(figsize=(6, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['0', '1'], yticklabels=['0', '1'])
plt.xlabel('Predicted')
plt.ylabel('True')

plt.show()

# 3. **Cross-validation ROC AUC for RandomForest**
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)
roc_auc_scores = []

# Loop through cross-validation splits and calculate ROC AUC
for train_idx, test_idx in cv.split(X1_tr, y1_tr):
    X_train, X_test = X1_tr.iloc[train_idx], X1_tr.iloc[test_idx]
    y_train, y_test = y1_tr.iloc[train_idx], y1_tr.iloc[test_idx]

    random_forest_pipeline.fit(X_train, y_train)
    y_pred_proba = random_forest_pipeline.predict_proba(X_test)[:, 1]  # Probability for class 1

    roc_auc = roc_auc_score(y_test, y_pred_proba)
    roc_auc_scores.append(roc_auc)

# Print the average ROC AUC score from cross-validation
print(f"Average ROC AUC (CV): {np.mean(roc_auc_scores):.4f} ± {np.std(roc_auc_scores):.4f}")

# Plot ROC curve for RandomForest
fpr, tpr, thresholds = roc_curve(y1_te, random_forest_pipeline.predict_proba(X1_te)[:, 1])
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate', fontsize=14)
plt.ylabel('True Positive Rate', fontsize=14)
plt.title('RandomForest ROC Curve', fontsize=16)
plt.legend(loc="lower right")
plt.show()

sns.heatmap(df1.corr(), annot=True, cmap='Blues')
plt.show()

plt.figure(figsize=(25,6))

a1 = plt.subplot2grid((1,3),(0,0))
a1.scatter(df1['Ia'], df1['Va'])
a1.set_title('Line a')
a1.set_xlabel('Ia')
a1.set_ylabel('Va')

plt.figure(figsize=(25,6))

a1 = plt.subplot2grid((1,3),(0,0))
a1.scatter(df1['Ib'], df1['Vb'])
a1.set_title('Line b')
a1.set_xlabel('Ib')
a1.set_ylabel('Vb')

plt.figure(figsize=(25,6))

a1 = plt.subplot2grid((1,3),(0,0))
a1.scatter(df1['Ic'], df1['Vc'])
a1.set_title('Line b')
a1.set_xlabel('Ic')
a1.set_ylabel('Vc')

import seaborn as sns
import matplotlib.pyplot as plt

# Select only numeric columns for correlation matrix
numeric_df = df2.select_dtypes(include=[np.number])

# Compute the correlation matrix
corr_matrix = numeric_df.corr()

# Plot the heatmap
plt.figure(figsize=(18, 12))
sns.heatmap(corr_matrix, annot=True, cmap='Blues', fmt='.2f', linewidths=0.5)
plt.title("Correlation Matrix of Numeric Features", fontsize=16)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Data: Classification results for each model
data = {
    "Model": [
        "RandomForest", "LogisticRegression", "SVM",
        "GradientBoosting", "KNN", "DecisionTree"
    ],
    "Accuracy": [0.9967, 0.6064, 0.9808, 0.9946, 0.9967, 0.9967],
    "Macro Avg Precision": [0.9965, 0.6039, 0.9826, 0.9945, 0.9964, 0.9966],
    "Macro Avg Recall": [0.9968, 0.6041, 0.9793, 0.9946, 0.9969, 0.9967],
    "Macro Avg F1-Score": [0.9966, 0.6040, 0.9806, 0.9945, 0.9966, 0.9966],
    "Weighted Avg Precision": [0.9967, 0.6068, 0.9813, 0.9946, 0.9967, 0.9967],
    "Weighted Avg Recall": [0.9967, 0.6064, 0.9808, 0.9946, 0.9967, 0.9967],
    "Weighted Avg F1-Score": [0.9967, 0.6066, 0.9808, 0.9946, 0.9967, 0.9967]
}

# Create the dataframe
df_results = pd.DataFrame(data)

# Select only the columns from 'Accuracy' to 'Weighted Avg F1-Score'
df_results = df_results[['Model', 'Accuracy', 'Macro Avg Precision', 'Macro Avg Recall',
                         'Macro Avg F1-Score', 'Weighted Avg Precision', 'Weighted Avg Recall', 'Weighted Avg F1-Score']]

# Convert numeric columns to float, ensuring the comparison works correctly
numeric_cols = df_results.columns[1:]  # all columns except 'Model'
df_results[numeric_cols] = df_results[numeric_cols].apply(pd.to_numeric, errors='coerce')

# Plotting the table without background
fig, ax = plt.subplots(figsize=(12, 6))  # Set the size of the figure

# Hide axes
ax.xaxis.set_visible(False)
ax.yaxis.set_visible(False)

# Create the table and add it to the axes
ax.table(cellText=df_results.values, colLabels=df_results.columns, loc='center', cellLoc='center')

# Save the table as an image with no background
plt.savefig('/content/classification_report_table_no_bg.png', bbox_inches='tight', transparent=True, dpi=300)

# Show the plot (Optional)
plt.show()

print("Table saved as 'classification_report_table_no_bg.png'")

X2, y2 = df2[feat], df2["faultType"]
X2_tr, X2_te, y2_tr, y2_te = train_test_split(X2, y2, test_size=0.2, random_state=1, stratify=y2)

classifier = Pipeline([
    ("scaler", StandardScaler()),
    ("clf", RandomForestClassifier(n_estimators=400, class_weight="balanced_subsample",
                                  random_state=1, n_jobs=-1))
])
classifier.fit(X2_tr, y2_tr)

y2_pred = classifier.predict(X2_te)
print("CLASSIFICATION REPORT")
print(classification_report(y2_te, y2_pred, digits=4))

# ---------- 2) Label mapping ----------
# New mapping function to correctly categorize all faults
def map_fault(row):
    G, C, B, A = int(row["G"]), int(row["C"]), int(row["B"]), int(row["A"])
    nph = A + B + C

    if G == 0 and nph == 0:
        return "No Fault"                      # No Fault
    if G == 1 and nph == 1 and A == 1:
        return "LG fault (A-G)"                # LG fault (A-G)
    if G == 1 and nph == 1 and B == 1:
        return "LG fault (B-G)"                # LG fault (B-G)
    if G == 1 and nph == 1 and C == 1:
        return "LG fault (C-G)"                # LG fault (C-G)
    if G == 0 and nph == 2 and A == 1 and B == 1:
        return "LL fault (A-B)"                # LL fault (A-B)
    if G == 0 and nph == 2 and A == 1 and C == 1:
        return "LL fault (A-C)"                # LL fault (A-C)
    if G == 0 and nph == 2 and B == 1 and C == 1:
        return "LL fault (B-C)"                # LL fault (B-C)
    if G == 1 and nph == 2 and A == 1 and B == 1:
        return "LLG Fault (A-B-G)"             # LLG fault (A-B-G)
    if G == 1 and nph == 2 and A == 1 and C == 1:
        return "LLG Fault (A-C-G)"             # LLG fault (A-C-G)
    if G == 1 and nph == 2 and B == 1 and C == 1:
        return "LLG Fault (B-C-G)"             # LLG fault (B-C-G)
    if G == 0 and nph == 3 and A == 1 and B == 1 and C == 1:
        return "LLL Fault"                     # LLL Fault
    if G == 1 and nph == 3 and A == 1 and B == 1 and C == 1:
        return "LLLG fault"                    # LLLG fault (Three-phase symmetrical fault)

    # If no condition matches, return None (for any unmapped rows)
    return None

df2["faultType"] = df2.apply(map_fault, axis=1)
df = df2.dropna(subset=["faultType"])  # drop rows where faultType is None

# Check mapping
print(df["faultType"].value_counts())

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix, f1_score
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier

# Define feature columns
feat_raw = ["Ia","Ib","Ic","Va","Vb","Vc"]

# ---------- 3) Feature engineering (helps LLL vs LLLG) ----------
def build_features(d):
    X = d[feat_raw].copy()
    # zero-sequence components (ground-sensitive)
    X["I0"] = (X["Ia"] + X["Ib"] + X["Ic"]) / 3.0
    X["V0"] = (X["Va"] + X["Vb"] + X["Vc"]) / 3.0
    # magnitudes
    X["I_mag"] = np.sqrt(X["Ia"]**2 + X["Ib"]**2 + X["Ic"]**2)
    X["V_mag"] = np.sqrt(X["Va"]**2 + X["Vb"]**2 + X["Vc"]**2)
    # imbalance indicators
    X["I_std"] = X[["Ia","Ib","Ic"]].std(axis=1)
    X["V_std"] = X[["Va","Vb","Vc"]].std(axis=1)
    return X

# Assuming df is your dataframe
X = build_features(df)
y = df["faultType"]

# ---------- 4) Split ----------
X_tr, X_te, y_tr, y_te = train_test_split(
    X, y, test_size=0.20, random_state=1, stratify=y
)

# ---------- 5) Define six models + Decision Tree & KNN ----------
models = {
    "LogReg": Pipeline([
        ("scaler", StandardScaler()),
        ("clf", LogisticRegression(max_iter=2000, class_weight="balanced"))
    ]),
    "SVC_RBF": Pipeline([
        ("scaler", StandardScaler()),
        ("clf", SVC(kernel="rbf", probability=True, class_weight="balanced"))
    ]),
    "RandomForest": RandomForestClassifier(
        n_estimators=400, max_depth=None, n_jobs=-1, random_state=1, class_weight="balanced_subsample"
    ),
    "ExtraTrees": ExtraTreesClassifier(
        n_estimators=500, max_depth=None, n_jobs=-1, random_state=1, class_weight="balanced"
    ),
    "GradBoost": GradientBoostingClassifier(random_state=1),
    "MLP": Pipeline([
        ("scaler", StandardScaler()),
        ("clf", MLPClassifier(hidden_layer_sizes=(128,64), activation="relu",
                              batch_size=256, max_iter=200, random_state=1))
    ]),
    "DecisionTree": DecisionTreeClassifier(random_state=1, class_weight="balanced"),
    "KNN": KNeighborsClassifier(n_neighbors=5)
}

# ---------- 6) Cross-validated comparison (macro F1) ----------
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)
cv_scores = {}
for name, model in models.items():
    score = cross_val_score(model, X_tr, y_tr, scoring="f1_macro", cv=cv, n_jobs=-1)
    cv_scores[name] = (score.mean(), score.std())

print("CV macro-F1 (mean ± std):")
for k, (m, s) in sorted(cv_scores.items(), key=lambda x: x[1][0], reverse=True):
    print(f"{k:12s}: {m:.4f} ± {s:.4f}")

# ---------- 7) Fit all, report test metrics with focus on LLL vs LLLG ----------
reports = {}
for name, model in models.items():
    model.fit(X_tr, y_tr)
    y_pred = model.predict(X_te)
    rep = classification_report(y_te, y_pred, digits=4, output_dict=True)
    reports[name] = rep
    print("\n", "=" * 60, f"\n{name} — TEST REPORT")
    print(classification_report(y_te, y_pred, digits=4))
    print("Confusion matrix:\n", confusion_matrix(y_te, y_pred))

# ---------- 8) Quick comparison for LLL vs LLLG only ----------
def cls_metric(rep, label, key="f1-score"):
    return rep.get(label, {}).get(key, np.nan)

summary = []
for name, rep in reports.items():
    summary.append({
        "model": name,
        "F1_LLL": cls_metric(rep, "LLL"),
        "F1_LLLG": cls_metric(rep, "LLLG"),
        "MacroF1": rep["macro avg"]["f1-score"],
        "Accuracy": rep["accuracy"],
    })

sum_df = pd.DataFrame(summary).sort_values(["F1_LLLG", "F1_LLL", "MacroF1"], ascending=False)
print("\nFocused comparison (higher is better):")
print(sum_df.to_string(index=False))



import matplotlib.pyplot as plt
import numpy as np

# Model names
models = ["LogReg", "SVC_RBF", "RandomForest", "ExtraTrees",
          "GradBoost", "MLP", "DecisionTree", "KNN"]

# Overall accuracy (from your reports)
accuracy = [0.6682, 0.8182, 0.9981, 0.9962, 0.9955, 0.8576, 0.9930, 0.7972]

# Macro F1-score (from your reports)
macro_f1 = [0.6125, 0.7772, 0.9978, 0.9957, 0.9949, 0.7998, 0.9919, 0.7573]

# X locations
x = np.arange(len(models))
width = 0.35

plt.figure(figsize=(10,6))
bars1 = plt.bar(x - width/2, accuracy, width, label="Accuracy", color="skyblue")
bars2 = plt.bar(x + width/2, macro_f1, width, label="Macro F1", color="salmon")

# Add value labels
for bar in bars1:
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
             f"{bar.get_height():.3f}", ha='center', fontsize=9, fontweight='bold')

for bar in bars2:
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
             f"{bar.get_height():.3f}", ha='center', fontsize=9, fontweight='bold')

# Formatting
plt.xticks(x, models, rotation=30, ha="right")
plt.ylabel("Score", fontsize=12, fontweight="bold")
plt.ylim(0, 1.1)
plt.legend()
plt.grid(axis="y", linestyle="--", alpha=0.6)

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Model names and their corresponding F1-scores (mean)
models = ['RandomForest', 'ExtraTrees', 'GradBoost', 'DecisionTree', 'MLP', 'KNN', 'SVC_RBF', 'LogReg']
f1_scores = [0.9969, 0.9950, 0.9935, 0.9918, 0.8124, 0.7896, 0.7643, 0.5930]

# Create a bar chart to display the F1-scores
plt.figure(figsize=(10, 6))
bars = plt.barh(models, f1_scores, color='skyblue')


plt.xlabel('Macro-F1 Score', fontsize=14, fontweight='bold')
plt.ylabel('Models', fontsize=14, fontweight='bold')
plt.xlim(0, 1)

# Adding values to the right edge of the bars
for bar in bars:
    plt.text(bar.get_width() - 0.02, bar.get_y() + bar.get_height() / 2,
             f'{bar.get_width():.4f}', va='center', ha='right', fontsize=12, fontweight='bold', color='black')

# Display the plot
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

cm = confusion_matrix(y_te, y_pred)
print("Confusion Matrix:\n", cm)

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score, StratifiedKFold



rf_model = RandomForestClassifier(n_estimators=500, max_depth=None, n_jobs=-1, random_state=1, class_weight="balanced_subsample")


cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)

# Perform 5-fold cross-validation on the RandomForest model
cv_scores = cross_val_score(rf_model, X, y, cv=cv, scoring='accuracy')

# Print the accuracy for each fold
print(f"Accuracy for each fold: {cv_scores}")

# Print the average accuracy
print(f"Average accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}")

# Redrawing the plot with fixed legend arguments
import matplotlib.pyplot as plt
import numpy as np

# Accuracy data for each fold
fold_accuracies = [0.9955499, 0.99872774, 0.99936387, 0.99872774, 0.99872774]

# Set up the plot
plt.figure(figsize=(10,6))
x = np.arange(1, 6)  # Fold numbers (1, 2, 3, 4, 5)

# Plot the accuracy for each fold
plt.plot(x, fold_accuracies, marker='o', linestyle='-', color='b', label='Accuracy per fold', linewidth=2)

# Plot the average accuracy as a horizontal line
avg_accuracy = np.mean(fold_accuracies)
plt.axhline(avg_accuracy, color='r', linestyle='--', label=f'Average Accuracy: {avg_accuracy:.4f}', linewidth=2)

# Adding labels and title

plt.xlabel('Fold', fontsize=14, fontweight='bold')
plt.ylabel('Accuracy', fontsize=14, fontweight='bold')

# Adding grid and setting axis limits
plt.grid(True, which='both', linestyle='--', linewidth=0.5)

# Fixing the legend properties
legend = plt.legend(fontsize=12)
for label in legend.get_texts():
    label.set_fontweight('bold')

# Show the plot
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.ensemble import RandomForestClassifier

# Example model (RandomForest)
model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model on the entire training set (replace with your own training data)
model.fit(X_tr, y_tr)

# Predict probabilities for the test set
y_probs = model.predict_proba(X_te)

# Classes and their corresponding average AUC values
classes = ['LG fault (A-G)', 'LL fault (B-C)', 'LLG Fault (A-B-G)', 'LLL Fault', 'LLLG fault', 'No Fault']

# Set up the plot
plt.figure(figsize=(10, 6))

# Plot the ROC curve for each class
for i, class_name in enumerate(classes):
    # Compute the ROC curve
    fpr, tpr, _ = roc_curve(y_te == class_name, y_probs[:, i])

    # Calculate the AUC
    auc = roc_auc_score(y_te == class_name, y_probs[:, i])

    # Plot the ROC curve for the current class
    plt.plot(fpr, tpr, linestyle='--', label=f'{class_name} (AUC = {auc:.4f})')

# Plot diagonal (chance level)
plt.plot([0, 1], [0, 1], linestyle='--', color='k')

# Adding labels and title

plt.xlabel('False Positive Rate', fontsize=14, fontweight='bold')
plt.ylabel('True Positive Rate', fontsize=14, fontweight='bold')

# Adding grid and legend with bold text
plt.grid(True, which='both', linestyle='--', linewidth=0.5)
legend = plt.legend(loc='lower right', fontsize=10)
for label in legend.get_texts():
    label.set_fontweight('bold')

# Show the plot
plt.tight_layout()
plt.show()

!pip install lime shap

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix, f1_score
from sklearn.ensemble import RandomForestClassifier
from lime import lime_tabular

# Define feature columns
feat_raw = ["Ia", "Ib", "Ic", "Va", "Vb", "Vc"]

# Feature engineering function
def build_features(d):
    X = d[feat_raw].copy()
    X["I0"] = (X["Ia"] + X["Ib"] + X["Ic"]) / 3.0
    X["V0"] = (X["Va"] + X["Vb"] + X["Vc"]) / 3.0
    X["I_mag"] = np.sqrt(X["Ia"]**2 + X["Ib"]**2 + X["Ic"]**2)
    X["V_mag"] = np.sqrt(X["Va"]**2 + X["Vb"]**2 + X["Vc"]**2)
    X["I_std"] = X[["Ia", "Ib", "Ic"]].std(axis=1)
    X["V_std"] = X[["Va", "Vb", "Vc"]].std(axis=1)
    return X

# Assuming df is your dataframe
X = build_features(df)
y = df["faultType"]

# Split data into training and test sets
X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.20, random_state=1, stratify=y)

# Fit the RandomForest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_tr, y_tr)

# LIME Explainer for RandomForest
explainer = lime_tabular.LimeTabularExplainer(
    training_data=X_tr.values,  # Training data
    training_labels=y_tr.values,  # Training labels
    mode='classification',  # Since it's a classification task
    feature_names=X_tr.columns,  # Feature names from the dataframe
    class_names=y_tr.unique(),  # Target class names
    discretize_continuous=True  # Discretize continuous features
)

# Select a specific instance for which we want to explain the prediction
instance = X_te.iloc[0]  # Example instance for which we want to explain the prediction

# Explain the prediction for the selected instance
explanation = explainer.explain_instance(instance.values, rf_model.predict_proba)

# Display the explanation
explanation.show_in_notebook(show_table=True, show_all=False)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from lime import lime_tabular
import matplotlib.pyplot as plt

# Define feature columns
feat_raw = ["Ia", "Ib", "Ic", "Va", "Vb", "Vc"]

# Feature engineering function
def build_features(d):
    X = d[feat_raw].copy()
    X["I0"] = (X["Ia"] + X["Ib"] + X["Ic"]) / 3.0
    X["V0"] = (X["Va"] + X["Vb"] + X["Vc"]) / 3.0
    X["I_mag"] = np.sqrt(X["Ia"]**2 + X["Ib"]**2 + X["Ic"]**2)
    X["V_mag"] = np.sqrt(X["Va"]**2 + X["Vb"]**2 + X["Vc"]**2)
    X["I_std"] = X[["Ia", "Ib", "Ic"]].std(axis=1)
    X["V_std"] = X[["Va", "Vb", "Vc"]].std(axis=1)
    return X

# Example dataframe 'df' (Assuming it's already loaded in your environment)
X = build_features(df)
y = df["faultType"]

# Split data into training and test sets
X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.20, random_state=1, stratify=y)

# Fit the RandomForest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_tr, y_tr)

# LIME Explainer for RandomForest
explainer = lime_tabular.LimeTabularExplainer(
    training_data=X_tr.values,  # Training data
    training_labels=y_tr.values,  # Training labels
    mode='classification',  # Since it's a classification task
    feature_names=X_tr.columns,  # Feature names from the dataframe
    class_names=y_tr.unique(),  # Target class names
    discretize_continuous=True  # Discretize continuous features
)

# Select a specific instance for which we want to explain the prediction
instance = X_te.iloc[0]  # Example instance for which we want to explain the prediction

# Get the explanation for the selected instance
explanation = explainer.explain_instance(instance.values, rf_model.predict_proba)

# Get the feature importance from the explanation
importance = explanation.as_list()

# Prepare a dictionary for feature values for this specific instance
instance_values = {feature: instance[feature] for feature in feat_raw if feature in instance.index}

# Ensure features with the highest importance are at the top
features, importance_values = zip(*importance)
sorted_features = [features[i] for i in np.argsort(importance_values)]
sorted_importance_values = sorted(importance_values)
sorted_instance_values = [instance_values[feature] if feature in instance_values else 'N/A' for feature in sorted_features]

# Plot the LIME explanation as a bar chart with feature values included
plt.figure(figsize=(12, 6))
bars = plt.barh(sorted_features, sorted_importance_values, color='skyblue')
plt.title('LIME Feature Importance for RandomForest Prediction', fontsize=16, fontweight='bold')
plt.xlabel('Feature Importance', fontsize=14, fontweight='bold')
plt.ylabel('Features', fontsize=14, fontweight='bold')

# Adding feature values to the chart
for i, feature in enumerate(sorted_features):
    feature_value = sorted_instance_values[i]  # Get the value of the feature in the selected instance
    plt.text(sorted_importance_values[i] + 0.001, i, f'{feature_value}', va='center', fontsize=12, fontweight='bold')

# Show the plot
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Model names
models = ["LogReg", "SVC_RBF", "RandomForest", "ExtraTrees",
          "GradBoost", "MLP", "DecisionTree", "KNN"]

# Accuracy values
accuracy = [0.6682, 0.8182, 0.9981, 0.9962, 0.9955, 0.8576, 0.9930, 0.7972]

# Macro F1 values
macro_f1 = [0.6125, 0.7772, 0.9978, 0.9957, 0.9949, 0.7998, 0.9919, 0.7573]

x = np.arange(len(models))
width = 0.35

plt.figure(figsize=(10,6))
bars1 = plt.bar(x - width/2, accuracy, width, label="Accuracy", color="skyblue")
bars2 = plt.bar(x + width/2, macro_f1, width, label="Macro F1", color="salmon")

# Custom offsets (stagger only the high accuracy models to avoid overlap)
offsets_acc = [0.01, 0.01, -0.04, 0.03, -0.03, 0.01, 0.04, 0.01]
offsets_f1  = [0.01, 0.01, -0.02, 0.04, -0.02, 0.01, 0.05, 0.01]

# Add value labels with staggered positions
for bar, off in zip(bars1, offsets_acc):
    h = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, h + off, f"{h:.3f}",
             ha='center', va='bottom', fontsize=9, fontweight='bold')

for bar, off in zip(bars2, offsets_f1):
    h = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, h + off, f"{h:.3f}",
             ha='center', va='bottom', fontsize=9, fontweight='bold')

plt.xticks(x, models, rotation=30, ha="right")
plt.ylabel("Score", fontsize=12, fontweight="bold")
plt.ylim(0, 1.1)
plt.legend(fontsize=9)
plt.grid(axis="y", linestyle="--", alpha=0.6)

plt.tight_layout()
plt.show()